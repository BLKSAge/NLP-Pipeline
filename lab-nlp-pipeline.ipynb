{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/ga-logo.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Lab: Build a Question Answer (QA) Pipeline for Financial Documents\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The goal of this lab is to build a Question Answer (QA) pipeline for financial documents that financial analysts can use to inform their research by processing tons of text based data quickly, and pulling out key insights.\n",
    "\n",
    "#### Steps:\n",
    "1. Build an initial pipeline using a single text document `financial_context`.\n",
    "2. Test a few pretrained models and select one to use for this task.\n",
    "3. Evaluate the performance of your model.\n",
    "4. Make recommendations for when if should and should not be used.\n",
    "5. _BONUS_: Extend the pipeline to a larger set of documents from the FinQA data.\n",
    "\n",
    "# Data\n",
    "In this lab you will data from the FinQA dataset. \n",
    "\n",
    "In the `data` folder you have a copy of the FinQA datasets. [Original FinQA Source](https://github.com/czyssrs/FinQA)\n",
    "\n",
    "For question 1, you will use one sample document, labeled `financial_context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample financial context from FinQA dataset\n",
    "financial_context = \"\"\"\n",
    "ITEM 7: MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
    "\n",
    "Net Sales\n",
    "Fiscal 2018 net sales increased 7% to $8,716 million compared to $8,140 million in fiscal 2017. The increase in net sales \n",
    "primarily reflects higher sales in our Convenience Stores and Retail segments, which increased 8% and 7%, respectively. \n",
    "The higher Convenience Stores sales were driven by contributions from acquisitions and a 1.9% increase in same-store \n",
    "merchandise sales. Retail sales benefited from a 4.3% increase in same-store sales, including 38% growth in digital \n",
    "sales, and sales from new and relocated stores, partly offset by the impact of closed stores. Wholesale segment sales \n",
    "increased 4% from fiscal 2017, which reflected the benefit of sales to new customers and higher sales to existing customers, \n",
    "partly offset by customer attrition.\n",
    "\n",
    "Gross Profit\n",
    "Gross profit increased 7% to $2,164 million in fiscal 2018 versus $2,019 million in fiscal 2017, reflecting higher sales \n",
    "and improved Wholesale segment gross profit rates. Consolidated gross profit rate for fiscal 2018 was 24.8%, equal to \n",
    "the fiscal 2017 rate, as the benefit of the improved Wholesale segment gross profit rate was offset by a lower Retail \n",
    "segment rate. The Wholesale segment gross profit rate improved 16 basis points to 14.5%, due primarily to a shift \n",
    "in business mix, including the benefit of acquisitions, and lower inventory shrink. The Retail segment rate decreased \n",
    "30 basis points to 27.0%, primarily reflecting a higher LIFO expense and increased promotional activity.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Build a QA pipeline \n",
    "\n",
    "1. Build a 'question-answering' pipeline using the Hugging Face pipeline and the [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad)\n",
    "2. Ask a few related questions of the sample document `financial_context` which should be passed into your pipeline as context \n",
    "\n",
    "\n",
    "\n",
    "**Questions**: \n",
    "```python \n",
    "    questions = [\n",
    "    \"What was the net sales amount in fiscal 2018?\",\n",
    "    \"By what percentage did fiscal 2018 net sales increase compared to fiscal 2017?\",\n",
    "    \"What was the gross profit in fiscal 2018?\",\n",
    "    \"What was the Wholesale segment gross profit rate?\"\n",
    "    ]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a QA pipeline using a pre-trained model using `distilbert-base-cased-distilled-squad`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask a question about the first question about the financial context \n",
    "question = \"What was the increase in net sales for fiscal 2018?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the QA pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the result (be sure to print the score as well as the result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did the model do on this question? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try additional questions\n",
    "questions = [\n",
    "\"What was the net sales amount in fiscal 2018?\",\n",
    "\"By what percentage did fiscal 2018 net sales increase compared to fiscal 2017?\",\n",
    "\"What was the gross profit in fiscal 2018?\",\n",
    "\"What was the Wholesale segment gross profit rate?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a for loop to iterate through the questions and call the QA pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did the model do on this question? \n",
    "Note your results may be different from the solutions. Evaluate your results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:  Test a few pretrained models and select one to use for this task\n",
    "\n",
    "-  Explore Hugging Face and select a few additional models to test for this task \n",
    "-  Feeling stuck? try the following \n",
    "   -  [bert-large-uncased-whole-word-masking-finetuned-squad](https://huggingface.co/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad)\n",
    "   -  [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the each question and evaluate the results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did the model do with the questions?\n",
    "**Reminder: questions**\n",
    "1. Net sales amount in fiscal 2018\n",
    "2. Percentage increase for fiscal 2018\n",
    "3. Gross profit in fiscal 2018\n",
    "4. Wholesale segment gross profit rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did the model do with the questions?\n",
    "**Reminder: questions**\n",
    "1. Net sales amount in fiscal 2018\n",
    "2. Percentage increase for fiscal 2018\n",
    "3. Gross profit in fiscal 2018\n",
    "4. Wholesale segment gross profit rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "#### a) Which model performed the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Besides accuracy what other factors are important to consider when evaluating the performance of a QA model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS Question:  Extend the pipeline \n",
    "-  Extend the pipeline to a larger set of documents from the FinQA data \n",
    "Steps:\n",
    "-  load the dataset using the helper code below \n",
    "-  Iterate through different contexts with a for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset \n",
    "In the `data` folder you have a copy of the FinQA datasets. The function below will load it into pandas. \n",
    "\n",
    "[Original FinQA Source](https://github.com/czyssrs/FinQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the dataset\n",
    "path = \"data/FinQA/\"\n",
    "\n",
    "# open and load the training data\n",
    "with open(f\"{path}/train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# convert to a pandas DataFrame\n",
    "train_dataset = pd.DataFrame(train_data)\n",
    "\n",
    "# print dataset info\n",
    "print(f\"Training dataset loaded: {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare context from a sample\n",
    "\n",
    "\n",
    "# test the QA pipeline on a few examples from the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: How could you extend this to a real world model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
